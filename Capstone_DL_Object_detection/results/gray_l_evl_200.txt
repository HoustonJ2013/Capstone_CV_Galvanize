Namespace(arch_decoder='c1_bilinear', arch_encoder='resnet34_dilated8', batch_size=1, ckpt='./ckpt', fc_dim=512, id='Grayscale2-resnet34_dilated8-c1_bilinear-ngpus1-batchSize8-imgSize384-segSize384-lr_encoder0.001-lr_decoder0.01-epoch5-decay0.0001', imgSize=-1, list_val='./data/Jingbo_gray_l_val.txt', num_class=150, num_val=-1, result='./result', root_img='./data/ADEChallengeData2016/images', root_seg='./data/ADEChallengeData2016/annotations', segSize=-1, suffix='_best.pth', visualize=0)
Loading weights for net_encoder
Loading weights for net_decoder
# samples: 200
[2017-12-28 03:53:08] iter 0, loss: 0.963299870491, accuracy: 0.751256691031
[2017-12-28 03:53:08] iter 1, loss: 0.979976892471, accuracy: 0.71451623419
[2017-12-28 03:53:09] iter 2, loss: 1.0484701395, accuracy: 0.63508396342
[2017-12-28 03:53:11] iter 3, loss: 0.459358841181, accuracy: 0.897613727446
[2017-12-28 03:53:13] iter 4, loss: 0.876785755157, accuracy: 0.670444493103
[2017-12-28 03:53:14] iter 5, loss: 0.727809667587, accuracy: 0.806821647235
[2017-12-28 03:53:14] iter 6, loss: 0.338074356318, accuracy: 0.947985264393
[2017-12-28 03:53:16] iter 7, loss: 0.683745265007, accuracy: 0.801591720242
[2017-12-28 03:53:17] iter 8, loss: 0.378304123878, accuracy: 0.914914703493
[2017-12-28 03:53:19] iter 9, loss: 0.3870870471, accuracy: 0.88188992346
[2017-12-28 03:53:21] iter 10, loss: 0.784581303596, accuracy: 0.779800847692
[2017-12-28 03:53:23] iter 11, loss: 0.584784328938, accuracy: 0.824775879585
[2017-12-28 03:53:25] iter 12, loss: 0.737067520618, accuracy: 0.794462068471
[2017-12-28 03:53:27] iter 13, loss: 0.695160984993, accuracy: 0.888389934697
[2017-12-28 03:53:28] iter 14, loss: 1.71088910103, accuracy: 0.598328322121
[2017-12-28 03:53:29] iter 15, loss: 1.29161643982, accuracy: 0.61475234842
[2017-12-28 03:53:29] iter 16, loss: 0.666143238544, accuracy: 0.785499812336
[2017-12-28 03:53:31] iter 17, loss: 0.820630669594, accuracy: 0.792859647242
[2017-12-28 03:53:32] iter 18, loss: 0.381825000048, accuracy: 0.8861640115
[2017-12-28 03:53:34] iter 19, loss: 0.922331750393, accuracy: 0.760241230334
[2017-12-28 03:53:34] iter 20, loss: 1.15573036671, accuracy: 0.658896111962
[2017-12-28 03:53:35] iter 21, loss: 0.91420763731, accuracy: 0.68672075084
[2017-12-28 03:53:37] iter 22, loss: 1.76652061939, accuracy: 0.470393060107
[2017-12-28 03:53:37] iter 23, loss: 0.840383410454, accuracy: 0.853532780685
[2017-12-28 03:53:39] iter 24, loss: 0.571891129017, accuracy: 0.829578503021
[2017-12-28 03:53:41] iter 25, loss: 0.908283293247, accuracy: 0.717889494582
[2017-12-28 03:53:42] iter 26, loss: 1.47003233433, accuracy: 0.579283162951
[2017-12-28 03:53:44] iter 27, loss: 2.24922990799, accuracy: 0.43136876933
[2017-12-28 03:53:46] iter 28, loss: 0.394371658564, accuracy: 0.865889930463
[2017-12-28 03:53:46] iter 29, loss: 2.39575314522, accuracy: 0.485170864716
[2017-12-28 03:53:47] iter 30, loss: 1.92640352249, accuracy: 0.317726956694
[2017-12-28 03:53:47] iter 31, loss: 1.08255255222, accuracy: 0.614712195122
[2017-12-28 03:53:48] iter 32, loss: 0.330668866634, accuracy: 0.871232849153
[2017-12-28 03:53:49] iter 33, loss: 1.58416748047, accuracy: 0.539889706858
[2017-12-28 03:53:51] iter 34, loss: 0.302824705839, accuracy: 0.915665666386
[2017-12-28 03:53:54] iter 35, loss: 0.619672477245, accuracy: 0.806203695913
[2017-12-28 03:53:54] iter 36, loss: 0.739127337933, accuracy: 0.797323301923
[2017-12-28 03:53:56] iter 37, loss: 0.517865240574, accuracy: 0.863142026489
[2017-12-28 03:53:58] iter 38, loss: 0.661531209946, accuracy: 0.806968055097
[2017-12-28 03:54:00] iter 39, loss: 0.618969261646, accuracy: 0.799716344449
[2017-12-28 03:54:00] iter 40, loss: 0.689571619034, accuracy: 0.748088988526
[2017-12-28 03:54:02] iter 41, loss: 0.615570664406, accuracy: 0.833141312728
[2017-12-28 03:54:03] iter 42, loss: 1.04693496227, accuracy: 0.631101898143
[2017-12-28 03:54:03] iter 43, loss: 0.471922725439, accuracy: 0.843256646056
[2017-12-28 03:54:04] iter 44, loss: 0.39239013195, accuracy: 0.892343937716
[2017-12-28 03:54:06] iter 45, loss: 1.05728363991, accuracy: 0.771236048678
[2017-12-28 03:54:06] iter 46, loss: 1.22142660618, accuracy: 0.646742489801
[2017-12-28 03:54:07] iter 47, loss: 1.48441827297, accuracy: 0.463503818204
[2017-12-28 03:54:08] iter 48, loss: 1.79637980461, accuracy: 0.570273522702
[2017-12-28 03:54:10] iter 49, loss: 0.922749936581, accuracy: 0.729668077523
[2017-12-28 03:54:12] iter 50, loss: 1.35888123512, accuracy: 0.672250893703
[2017-12-28 03:54:13] iter 51, loss: 2.39374470711, accuracy: 0.276199288483
[2017-12-28 03:54:15] iter 52, loss: 0.674154341221, accuracy: 0.745345254668
[2017-12-28 03:54:17] iter 53, loss: 1.48251187801, accuracy: 0.622726156015
[2017-12-28 03:54:17] iter 54, loss: 0.384307354689, accuracy: 0.928318472143
[2017-12-28 03:54:20] iter 55, loss: 0.443766802549, accuracy: 0.889218975841
[2017-12-28 03:54:22] iter 56, loss: 0.412938088179, accuracy: 0.943767818107
[2017-12-28 03:54:24] iter 57, loss: 0.844347178936, accuracy: 0.745386989465
[2017-12-28 03:54:24] iter 58, loss: 0.320533841848, accuracy: 0.987832863589
[2017-12-28 03:54:26] iter 59, loss: 0.98667550087, accuracy: 0.722948267013
[2017-12-28 03:54:28] iter 60, loss: 0.828668773174, accuracy: 0.66226723789
[2017-12-28 03:54:30] iter 61, loss: 1.0961356163, accuracy: 0.660584769841
[2017-12-28 03:54:31] iter 62, loss: 1.18813586235, accuracy: 0.668031189084
[2017-12-28 03:54:33] iter 63, loss: 0.430297881365, accuracy: 0.865629966416
[2017-12-28 03:54:33] iter 64, loss: 0.702563524246, accuracy: 0.800160096924
[2017-12-28 03:54:36] iter 65, loss: 1.96870291233, accuracy: 0.32213687007
[2017-12-28 03:54:38] iter 66, loss: 0.593851685524, accuracy: 0.852564461594
[2017-12-28 03:54:40] iter 67, loss: 0.958219647408, accuracy: 0.704486570328
[2017-12-28 03:54:42] iter 68, loss: 0.4313339293, accuracy: 0.858605137326
[2017-12-28 03:54:43] iter 69, loss: 0.245091438293, accuracy: 0.932972104494
[2017-12-28 03:54:45] iter 70, loss: 0.804260075092, accuracy: 0.756911208151
[2017-12-28 03:54:46] iter 71, loss: 0.768477976322, accuracy: 0.758082052769
[2017-12-28 03:54:48] iter 72, loss: 0.856879591942, accuracy: 0.800722459499
[2017-12-28 03:54:49] iter 73, loss: 2.4354300499, accuracy: 0.352992995574
[2017-12-28 03:54:49] iter 74, loss: 0.562387168407, accuracy: 0.852669753086
[2017-12-28 03:54:51] iter 75, loss: 2.36559677124, accuracy: 0.246070624289
[2017-12-28 03:54:53] iter 76, loss: 1.20038688183, accuracy: 0.722981695059
[2017-12-28 03:54:54] iter 77, loss: 1.12092542648, accuracy: 0.655566821424
[2017-12-28 03:54:54] iter 78, loss: 0.773515880108, accuracy: 0.694091204115
[2017-12-28 03:54:55] iter 79, loss: 0.891233026981, accuracy: 0.780234576694
[2017-12-28 03:54:55] iter 80, loss: 0.744162559509, accuracy: 0.834217706559
[2017-12-28 03:54:57] iter 81, loss: 0.93355178833, accuracy: 0.739259805226
[2017-12-28 03:54:59] iter 82, loss: 1.50982832909, accuracy: 0.537988751101
[2017-12-28 03:55:00] iter 83, loss: 1.66795516014, accuracy: 0.585563095531
[2017-12-28 03:55:00] iter 84, loss: 0.580114781857, accuracy: 0.800273597811
[2017-12-28 03:55:01] iter 85, loss: 1.13442826271, accuracy: 0.625127937977
[2017-12-28 03:55:03] iter 86, loss: 0.44125148654, accuracy: 0.894248057828
[2017-12-28 03:55:05] iter 87, loss: 0.42368799448, accuracy: 0.860113093655
[2017-12-28 03:55:06] iter 88, loss: 0.718596220016, accuracy: 0.823947630815
[2017-12-28 03:55:08] iter 89, loss: 1.37430346012, accuracy: 0.622935120158
[2017-12-28 03:55:08] iter 90, loss: 2.12689113617, accuracy: 0.453556109165
[2017-12-28 03:55:10] iter 91, loss: 0.661114513874, accuracy: 0.809044225257
[2017-12-28 03:55:11] iter 92, loss: 0.612255096436, accuracy: 0.888990208882
[2017-12-28 03:55:12] iter 93, loss: 0.640597999096, accuracy: 0.746536493846
[2017-12-28 03:55:13] iter 94, loss: 0.993622660637, accuracy: 0.760720241732
[2017-12-28 03:55:15] iter 95, loss: 2.33953952789, accuracy: 0.422367276804
[2017-12-28 03:55:17] iter 96, loss: 1.5765184164, accuracy: 0.616639665414
[2017-12-28 03:55:18] iter 97, loss: 0.641509771347, accuracy: 0.878045532036
[2017-12-28 03:55:20] iter 98, loss: 1.40698754787, accuracy: 0.75690776312
[2017-12-28 03:55:22] iter 99, loss: 3.47775483131, accuracy: 0.198246668633
[2017-12-28 03:55:22] iter 100, loss: 1.93264436722, accuracy: 0.525981183325
[2017-12-28 03:55:23] iter 101, loss: 2.34784197807, accuracy: 0.354419099719
[2017-12-28 03:55:25] iter 102, loss: 1.37464821339, accuracy: 0.708233913144
[2017-12-28 03:55:25] iter 103, loss: 1.18108332157, accuracy: 0.613783750078
[2017-12-28 03:55:27] iter 104, loss: 0.629795432091, accuracy: 0.832168989933
[2017-12-28 03:55:28] iter 105, loss: 0.638274371624, accuracy: 0.803096953609
[2017-12-28 03:55:31] iter 106, loss: 2.63454461098, accuracy: 0.380318421023
[2017-12-28 03:55:31] iter 107, loss: 0.46893838048, accuracy: 0.959777466447
[2017-12-28 03:55:33] iter 108, loss: 0.800261795521, accuracy: 0.790100074436
[2017-12-28 03:55:35] iter 109, loss: 1.05818581581, accuracy: 0.707561895297
[2017-12-28 03:55:36] iter 110, loss: 0.155092775822, accuracy: 0.969684473099
[2017-12-28 03:55:38] iter 111, loss: 0.273028463125, accuracy: 0.913621224629
[2017-12-28 03:55:40] iter 112, loss: 1.43600726128, accuracy: 0.593642832147
[2017-12-28 03:55:41] iter 113, loss: 0.833843827248, accuracy: 0.75498927287
[2017-12-28 03:55:42] iter 114, loss: 1.27980673313, accuracy: 0.70476506343
[2017-12-28 03:55:44] iter 115, loss: 0.915445804596, accuracy: 0.681647853039
[2017-12-28 03:55:45] iter 116, loss: 0.153659164906, accuracy: 0.966910678251
[2017-12-28 03:55:47] iter 117, loss: 1.06631052494, accuracy: 0.724554921061
[2017-12-28 03:55:48] iter 118, loss: 0.994189560413, accuracy: 0.608276819324
[2017-12-28 03:55:50] iter 119, loss: 0.845599234104, accuracy: 0.691372608795
[2017-12-28 03:55:52] iter 120, loss: 1.54261147976, accuracy: 0.568755835633
[2017-12-28 03:55:53] iter 121, loss: 1.10567474365, accuracy: 0.662940628524
[2017-12-28 03:55:53] iter 122, loss: 1.17657876015, accuracy: 0.591991442656
[2017-12-28 03:55:54] iter 123, loss: 0.270944952965, accuracy: 0.952690706261
[2017-12-28 03:55:56] iter 124, loss: 0.539203763008, accuracy: 0.849539584372
[2017-12-28 03:55:58] iter 125, loss: 1.42563521862, accuracy: 0.588027517139
[2017-12-28 03:55:59] iter 126, loss: 0.407238095999, accuracy: 0.905635671842
[2017-12-28 03:55:59] iter 127, loss: 0.480064302683, accuracy: 0.880810680392
[2017-12-28 03:56:00] iter 128, loss: 0.199057474732, accuracy: 0.955073897077
[2017-12-28 03:56:02] iter 129, loss: 0.820170760155, accuracy: 0.729045628558
[2017-12-28 03:56:04] iter 130, loss: 0.328211992979, accuracy: 0.923995548201
[2017-12-28 03:56:05] iter 131, loss: 0.833759009838, accuracy: 0.821387262344
[2017-12-28 03:56:05] iter 132, loss: 0.787880003452, accuracy: 0.760331075997
[2017-12-28 03:56:07] iter 133, loss: 0.326501339674, accuracy: 0.933945533294
[2017-12-28 03:56:09] iter 134, loss: 0.368491590023, accuracy: 0.88425772651
[2017-12-28 03:56:12] iter 135, loss: 0.74481767416, accuracy: 0.811794785056
[2017-12-28 03:56:13] iter 136, loss: 0.32672137022, accuracy: 0.887912310189
[2017-12-28 03:56:15] iter 137, loss: 1.51451396942, accuracy: 0.579100183977
[2017-12-28 03:56:17] iter 138, loss: 1.2360496521, accuracy: 0.552422376267
[2017-12-28 03:56:17] iter 139, loss: 0.241442203522, accuracy: 0.932627617031
[2017-12-28 03:56:19] iter 140, loss: 0.935042977333, accuracy: 0.639593996603
[2017-12-28 03:56:20] iter 141, loss: 1.2193980217, accuracy: 0.58436761721
[2017-12-28 03:56:22] iter 142, loss: 1.03795802593, accuracy: 0.596795731099
[2017-12-28 03:56:22] iter 143, loss: 1.2325387001, accuracy: 0.56720600342
[2017-12-28 03:56:24] iter 144, loss: 0.576836824417, accuracy: 0.82387394771
[2017-12-28 03:56:26] iter 145, loss: 0.223395168781, accuracy: 0.952766044222
[2017-12-28 03:56:27] iter 146, loss: 1.1626265049, accuracy: 0.69494007683
[2017-12-28 03:56:27] iter 147, loss: 0.624575078487, accuracy: 0.78195307938
[2017-12-28 03:56:28] iter 148, loss: 1.95703351498, accuracy: 0.288307706124
[2017-12-28 03:56:29] iter 149, loss: 0.66987401247, accuracy: 0.799323737436
[2017-12-28 03:56:30] iter 150, loss: 1.21810734272, accuracy: 0.657844265439
[2017-12-28 03:56:31] iter 151, loss: 0.883492708206, accuracy: 0.691134475133
[2017-12-28 03:56:31] iter 152, loss: 1.15332829952, accuracy: 0.681789979176
[2017-12-28 03:56:33] iter 153, loss: 0.37775939703, accuracy: 0.926436721827
[2017-12-28 03:56:34] iter 154, loss: 0.323879987001, accuracy: 0.894733572505
[2017-12-28 03:56:36] iter 155, loss: 0.988790631294, accuracy: 0.741349903554
[2017-12-28 03:56:38] iter 156, loss: 0.399322420359, accuracy: 0.973064500229
[2017-12-28 03:56:39] iter 157, loss: 0.43471762538, accuracy: 0.845546217238
[2017-12-28 03:56:41] iter 158, loss: 0.383659541607, accuracy: 0.888659549435
[2017-12-28 03:56:43] iter 159, loss: 1.12635791302, accuracy: 0.695575337477
[2017-12-28 03:56:45] iter 160, loss: 0.782541930676, accuracy: 0.861389193737
[2017-12-28 03:56:45] iter 161, loss: 2.96668958664, accuracy: 0.195562782407
[2017-12-28 03:56:45] iter 162, loss: 1.35971832275, accuracy: 0.61658589964
[2017-12-28 03:56:47] iter 163, loss: 0.539370000362, accuracy: 0.85070183079
[2017-12-28 03:56:48] iter 164, loss: 0.988664209843, accuracy: 0.840394270286
[2017-12-28 03:56:48] iter 165, loss: 1.66205787659, accuracy: 0.539026645661
[2017-12-28 03:56:49] iter 166, loss: 0.732410013676, accuracy: 0.869799742668
[2017-12-28 03:56:51] iter 167, loss: 0.609706819057, accuracy: 0.82583478669
[2017-12-28 03:56:53] iter 168, loss: 0.262200385332, accuracy: 0.889125071171
[2017-12-28 03:56:55] iter 169, loss: 0.614925146103, accuracy: 0.833987110071
[2017-12-28 03:56:56] iter 170, loss: 0.438365191221, accuracy: 0.980434605681
[2017-12-28 03:56:56] iter 171, loss: 1.07077538967, accuracy: 0.569388196742
[2017-12-28 03:56:58] iter 172, loss: 2.00748753548, accuracy: 0.515752719531
[2017-12-28 03:56:59] iter 173, loss: 1.20462059975, accuracy: 0.723462993982
[2017-12-28 03:56:59] iter 174, loss: 2.04231405258, accuracy: 0.421867881549
[2017-12-28 03:57:00] iter 175, loss: 2.802079916, accuracy: 0.356638057427
[2017-12-28 03:57:02] iter 176, loss: 1.37374460697, accuracy: 0.49857341083
[2017-12-28 03:57:03] iter 177, loss: 1.22932326794, accuracy: 0.509623383924
[2017-12-28 03:57:04] iter 178, loss: 1.73088777065, accuracy: 0.457582238517
[2017-12-28 03:57:05] iter 179, loss: 1.21371459961, accuracy: 0.680053491099
[2017-12-28 03:57:07] iter 180, loss: 0.33716738224, accuracy: 0.905607916641
[2017-12-28 03:57:08] iter 181, loss: 0.487871050835, accuracy: 0.857174330406
[2017-12-28 03:57:09] iter 182, loss: 2.38086533546, accuracy: 0.454681154576
[2017-12-28 03:57:11] iter 183, loss: 0.743699789047, accuracy: 0.78244783233
[2017-12-28 03:57:13] iter 184, loss: 0.527474462986, accuracy: 0.846735712431
[2017-12-28 03:57:14] iter 185, loss: 0.396253228188, accuracy: 0.921765653062
[2017-12-28 03:57:16] iter 186, loss: 0.531122624874, accuracy: 0.774200427562
[2017-12-28 03:57:18] iter 187, loss: 0.384655535221, accuracy: 0.888747271934
[2017-12-28 03:57:20] iter 188, loss: 0.690103471279, accuracy: 0.835995228727
[2017-12-28 03:57:20] iter 189, loss: 1.86638224125, accuracy: 0.232924045166
[2017-12-28 03:57:22] iter 190, loss: 0.659693717957, accuracy: 0.811419033473
[2017-12-28 03:57:24] iter 191, loss: 0.214805051684, accuracy: 0.95192567244
[2017-12-28 03:57:26] iter 192, loss: 1.21084702015, accuracy: 0.676440744847
[2017-12-28 03:57:28] iter 193, loss: 0.964076220989, accuracy: 0.451882358115
[2017-12-28 03:57:29] iter 194, loss: 0.162205547094, accuracy: 0.957353329297
[2017-12-28 03:57:29] iter 195, loss: 0.666776895523, accuracy: 0.903737952592
[2017-12-28 03:57:31] iter 196, loss: 1.36637163162, accuracy: 0.693968714348
[2017-12-28 03:57:33] iter 197, loss: 4.89141178131, accuracy: 0.183287311707
[2017-12-28 03:57:33] iter 198, loss: 1.83018755913, accuracy: 0.369289360668
[2017-12-28 03:57:34] iter 199, loss: 0.231309473515, accuracy: 0.937754992071
class [0], IoU: 0.648750901222
class [1], IoU: 0.756187736988
class [2], IoU: 0.89645922184
class [3], IoU: 0.631575286388
class [4], IoU: 0.739500999451
class [5], IoU: 0.7132563591
class [6], IoU: 0.79560303688
class [7], IoU: 0.669820547104
class [8], IoU: 0.499686151743
class [9], IoU: 0.513758718967
class [10], IoU: 0.426814198494
class [11], IoU: 0.515522122383
class [12], IoU: 0.68529945612
class [13], IoU: 0.353771209717
class [14], IoU: 0.158734366298
class [15], IoU: 0.348719447851
class [16], IoU: 0.702913582325
class [17], IoU: 0.35025665164
class [18], IoU: 0.557457685471
class [19], IoU: 0.282204270363
class [20], IoU: 0.767155528069
class [21], IoU: 0.650497436523
class [22], IoU: 0.656424999237
class [23], IoU: 0.198984235525
class [24], IoU: 0.456249684095
class [25], IoU: 0.130999669433
class [26], IoU: 0.707262217999
class [27], IoU: 0.351232886314
class [28], IoU: 0.0918536856771
class [29], IoU: 0.0
class [30], IoU: 0.0419810824096
class [31], IoU: 0.0992505699396
class [32], IoU: 0.315611928701
class [33], IoU: 0.206767141819
class [34], IoU: 0.316597849131
class [35], IoU: 0.174895375967
class [36], IoU: 0.341907441616
class [37], IoU: 0.511353254318
class [38], IoU: 0.0
class [39], IoU: 0.246741265059
class [40], IoU: 0.0980593264103
class [41], IoU: 0.276106417179
class [42], IoU: 0.0784603357315
class [43], IoU: 0.144596651196
class [44], IoU: 0.00262890476733
class [45], IoU: 0.573678672314
class [46], IoU: 0.0609946213663
class [47], IoU: 0.487363189459
class [48], IoU: 0.447671741247
class [49], IoU: 0.717967510223
class [50], IoU: 0.031965136528
class [51], IoU: 0.055638037622
class [52], IoU: 0.0370179899037
class [53], IoU: 0.054256208241
class [54], IoU: 0.0
class [55], IoU: 0.0
class [56], IoU: 0.824963271618
class [57], IoU: 0.38670450449
class [58], IoU: 0.42428457737
class [59], IoU: 0.509142696857
class [60], IoU: 0.0
class [61], IoU: 0.313913345337
class [62], IoU: 0.210334256291
class [63], IoU: 0.0899790450931
class [64], IoU: 0.137872576714
class [65], IoU: 0.53370064497
class [66], IoU: 0.140697583556
class [67], IoU: 0.311799585819
class [68], IoU: 0.0
class [69], IoU: 0.0218165889382
class [70], IoU: 0.406082421541
class [71], IoU: 0.0195983164012
class [72], IoU: 0.435652494431
class [73], IoU: 0.52810972929
class [74], IoU: 0.222737655044
class [75], IoU: 0.2484395504
class [76], IoU: 0.232217848301
class [77], IoU: 0.134687781334
class [78], IoU: 0.0
class [79], IoU: 0.0
class [80], IoU: 0.0
class [81], IoU: 0.0558463595808
class [82], IoU: 0.127947464585
class [83], IoU: 0.389121949673
class [84], IoU: 0.410152763128
class [85], IoU: 0.353521049023
class [86], IoU: 0.0971521064639
class [87], IoU: 0.0174845568836
class [88], IoU: 0.0
class [89], IoU: 0.156543210149
class [90], IoU: 0.0
class [91], IoU: 0.0
class [92], IoU: 0.0
class [93], IoU: 0.0405855216086
class [94], IoU: 0.0
class [95], IoU: 0.0
class [96], IoU: 0.0307948682457
class [97], IoU: 0.249440327287
class [98], IoU: 0.331546723843
class [99], IoU: 0.0902365371585
class [100], IoU: 0.0605704896152
class [101], IoU: 0.0
class [102], IoU: 0.0
class [103], IoU: 0.0
class [104], IoU: 0.0
class [105], IoU: 0.0
class [106], IoU: 0.0
class [107], IoU: 0.793211579323
class [108], IoU: 0.00674455007538
class [109], IoU: 0.0
class [110], IoU: 0.00686148693785
class [111], IoU: 0.0
class [112], IoU: 0.0
class [113], IoU: 0.0
class [114], IoU: 0.0
class [115], IoU: 0.0
class [116], IoU: 0.565650820732
class [117], IoU: 0.421218633652
class [118], IoU: 0.0
class [119], IoU: 0.0
class [120], IoU: 0.0
class [121], IoU: 0.0
class [122], IoU: 0.110819809139
class [123], IoU: 0.131095334888
class [124], IoU: 0.808598518372
class [125], IoU: 0.0282215345651
class [126], IoU: 0.608561098576
class [127], IoU: 0.428047001362
class [128], IoU: 0.0
class [129], IoU: 0.0
class [130], IoU: 0.0688745826483
class [131], IoU: 0.0
class [132], IoU: 0.0065334723331
class [133], IoU: 0.0
class [134], IoU: 0.0171458926052
class [135], IoU: 0.132221713662
class [136], IoU: 0.0
class [137], IoU: 0.0507129728794
class [138], IoU: 0.000931222573854
class [139], IoU: 0.271777927876
class [140], IoU: 0.0320026762784
class [141], IoU: 0.0
class [142], IoU: 0.0338393896818
class [143], IoU: 0.0
class [144], IoU: 0.187509506941
class [145], IoU: 0.0
class [146], IoU: 0.0
class [147], IoU: 0.00352965970524
class [148], IoU: 0.0
class [149], IoU: 0.0
[Eval Summary]:
Loss: 0.992568726689, Mean IoU: 0.2256, Accurarcy: 72.92%
Evaluation Done!
